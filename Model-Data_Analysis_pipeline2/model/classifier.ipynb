{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <span style=\"font-family: 'Playfair Display', serif; font-size: 24px; font-weight: bold;\">\n",
    "    Training an MLP Classifier with TransE Embeddings for Price Group Prediction\n",
    "  </span>\n",
    "</div>\n",
    "\n",
    "___\n",
    "\n",
    "In this notebook, we utilize embeddings generated by the `transE.ipynb` notebook to train a Multi-Layer Perceptron (MLP) for predicting price groups. The main steps include:\n",
    "\n",
    "- Data Acquisition: We obtain the embeddings created by the TransE model, which are stored in a previous notebook (transE.ipynb).\n",
    "- Target Definition: The price group, which serves as the target variable for our prediction model, is defined and prepared.\n",
    "- Model Design and Training: We design and train an MLP model using the TransE embeddings as input features and the price group as the target.\n",
    "- Evaluation: The trained MLP model is evaluated to assess its performance in predicting the price group.\n",
    "- Integration and Application: The trained model is integrated into the pipeline for further use in analysis and decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchkge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load pre-processed datasets and a pre-trained model from pickle files. It includes the training data (`train`), knowledge graph training data (`kg_train`), test features (`test_X`), test labels (`test_y`), and the model (`model`). These are used for further training, analysis, and prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./objects/train.pkl', 'rb') as outp:\n",
    "    train = pickle.load(outp)\n",
    "\n",
    "with open('./objects/kg_train.pkl', 'rb') as outp:\n",
    "    kg_train = pickle.load(outp)\n",
    "\n",
    "with open('./objects/test_X.pkl', 'rb') as outp:\n",
    "    test_X = pickle.load(outp)\n",
    "\n",
    "with open('./objects/test_y.pkl', 'rb') as outp:\n",
    "    test_y = pickle.load(outp)\n",
    "\n",
    "with open('./objects/model.pkl', 'rb') as outp:\n",
    "    model = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "\n",
    "Now, we filter and process training and testing data for an MLP model. Firts, we extract embeddings related to the 'price_discretized' relationship, loads them in batches, and concatenates them into a tensor. It then converts both the features (`train_X`) and target values (`train_y`) into PyTorch tensors for training.\n",
    "\n",
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000 \n",
    "\n",
    "filtered_items = train[train['rel'] == 'http://example.org/apartment/price_discretized']['from']\n",
    "\n",
    "train_embedding_list = [np.load(f'./train_X/batch_{start}.npy') for start in range(0, len(filtered_items), batch_size)]\n",
    "train_embedding_list = np.concatenate(train_embedding_list, axis=0)\n",
    "train_embedding_tensor = torch.tensor(train_embedding_list, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_embedding_tensor\n",
    "train_y = train[train['rel'] == 'http://example.org/apartment/price_discretized']['to']\n",
    "train_y = torch.tensor(train_y.astype(float).array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_items = test_X['from']\n",
    "\n",
    "test_embedding_list = [np.load(f'./test_X/batch_{start}.npy') for start in range(0, len(filtered_items), batch_size)]\n",
    "test_embedding_list = np.concatenate(test_embedding_list, axis=0)\n",
    "test_embedding_tensor = torch.tensor(test_embedding_list, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_embedding_tensor\n",
    "test_y = torch.tensor(test_y.astype(float).array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Training:\n",
    "\n",
    "Next, we define the MLP model, train it on the training data over 80 epochs, and evaluate its accuracy on both the training and test datasets using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition: MLP Classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, train_X, train_y, test_X, test_y, input_size, hidden_size=128, output_size=4, lr=0.001, num_epochs=80):\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y.long()\n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y.long()\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        self.model = MLPClassifier(input_size, hidden_size, output_size)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(self.train_X)\n",
    "            loss = self.criterion(outputs, self.train_y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{self.num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = self.model(self.train_X)\n",
    "            train_predictions = torch.argmax(train_outputs, dim=1)\n",
    "            train_accuracy = accuracy_score(self.train_y.numpy(), train_predictions.numpy())\n",
    "\n",
    "            test_outputs = self.model(self.test_X)\n",
    "            test_predictions = torch.argmax(test_outputs, dim=1)\n",
    "            test_accuracy = accuracy_score(self.test_y.numpy(), test_predictions.numpy())\n",
    "\n",
    "        print(f'Train Accuracy: {train_accuracy * 100:.2f}%')\n",
    "        print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "        return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_X.shape[1]\n",
    "trainer = Trainer(train_X, train_y, test_X, test_y, input_size)\n",
    "trainer.train()\n",
    "test_predictions = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(test_predictions)\n",
    "x.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(test_y, test_predictions)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "# Add labels to the plot\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, the model is not the most effective predictor. While it does perform better than a random baseline (which would be 25% accuracy), it rarely predicts the two most expensive classes accurately and often performs poorly when it does.\n",
    "\n",
    "Through experimentation, we have determined that these are the best results achievable with the current data before the model begins to overfit. This outcome could be attributed to several factors, including the inherent unpredictability of the data (though this is unlikely), an insufficient number of data instances, or a suboptimal embedding method (TransE), potentially due to an inadequately small embedding dimension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
