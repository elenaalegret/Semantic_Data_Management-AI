{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <span style=\"font-family: 'Playfair Display', serif; font-size: 24px; font-weight: bold;\">\n",
    "    Dataset Explotation\n",
    "  </span>\n",
    "</div>\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "In this notebook,  ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 09:52:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/03 09:52:36 WARN DependencyUtils: Local jar /Users/elenaalegretregalado/Desktop/en_curs/Semantic_Data_Management-AI/data_preparation_pipeline/duckdb.jar does not exist, skipping.\n",
      "24/06/03 09:52:36 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/06/03 09:52:36 INFO SparkContext: OS info Mac OS X, 14.4.1, x86_64\n",
      "24/06/03 09:52:36 INFO SparkContext: Java version 1.8.0_401\n",
      "24/06/03 09:52:36 INFO ResourceUtils: ==============================================================\n",
      "24/06/03 09:52:36 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/06/03 09:52:36 INFO ResourceUtils: ==============================================================\n",
      "24/06/03 09:52:36 INFO SparkContext: Submitted application: DataExploration\n",
      "24/06/03 09:52:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/06/03 09:52:36 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/06/03 09:52:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/06/03 09:52:36 INFO SecurityManager: Changing view acls to: elenaalegretregalado\n",
      "24/06/03 09:52:36 INFO SecurityManager: Changing modify acls to: elenaalegretregalado\n",
      "24/06/03 09:52:36 INFO SecurityManager: Changing view acls groups to: \n",
      "24/06/03 09:52:36 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/06/03 09:52:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: elenaalegretregalado; groups with view permissions: EMPTY; users with modify permissions: elenaalegretregalado; groups with modify permissions: EMPTY\n",
      "24/06/03 09:52:37 INFO Utils: Successfully started service 'sparkDriver' on port 58684.\n",
      "24/06/03 09:52:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/03 09:52:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/03 09:52:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/06/03 09:52:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/06/03 09:52:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/03 09:52:37 INFO DiskBlockManager: Created local directory at /private/var/folders/zw/r9ly271s6rq9xxqqj4ldc_740000gn/T/blockmgr-713be0bc-1106-4819-bb2d-4722fcbb986d\n",
      "24/06/03 09:52:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "24/06/03 09:52:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/06/03 09:52:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/06/03 09:52:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/06/03 09:52:38 ERROR SparkContext: Failed to add duckdb.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/elenaalegretregalado/Desktop/en_curs/Semantic_Data_Management-AI/data_preparation_pipeline/duckdb.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:526)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/06/03 09:52:38 INFO Executor: Starting executor ID driver on host mbp-de-elena.lan\n",
      "24/06/03 09:52:38 INFO Executor: OS info Mac OS X, 14.4.1, x86_64\n",
      "24/06/03 09:52:38 INFO Executor: Java version 1.8.0_401\n",
      "24/06/03 09:52:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/06/03 09:52:38 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6934c2e7 for default.\n",
      "24/06/03 09:52:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58686.\n",
      "24/06/03 09:52:38 INFO NettyBlockTransferService: Server created on mbp-de-elena.lan:58686\n",
      "24/06/03 09:52:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/06/03 09:52:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, mbp-de-elena.lan, 58686, None)\n",
      "24/06/03 09:52:38 INFO BlockManagerMasterEndpoint: Registering block manager mbp-de-elena.lan:58686 with 366.3 MiB RAM, BlockManagerId(driver, mbp-de-elena.lan, 58686, None)\n",
      "24/06/03 09:52:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, mbp-de-elena.lan, 58686, None)\n",
      "24/06/03 09:52:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, mbp-de-elena.lan, 58686, None)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import duckdb\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## Connection to formatted database\n",
    "jdbc_url = 'jdbc:duckdb:./../data/formatted_zone/barcelona.db'\n",
    "driver = \"org.duckdb.DuckDBDriver\"\n",
    "\n",
    "# SparkSession inicialitzation\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars\", \"duckdb.jar\") \\\n",
    "    .appName(\"DataExploration\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F2F2; padding: 10px;\">\n",
    "    <div style=\"text-align: center;\">\n",
    "        <span style=\"font-family: 'Playfair Display', serif; font-size: 20px; font-weight: bold; color: black;\">\n",
    "            Criminal Dataset\n",
    "        </span>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "#### DataFrame Schema\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color: #01571B; background-color: #C8E5C6;\">\n",
    "    \n",
    "The DataFrame comprises various groups of columns, each providing distinct details related to property listings:\n",
    "\n",
    "- The DataFrame has **2289 rows** and **16 columns**. \n",
    "\n",
    "\n",
    " - **any**: decimal(20,0) (nullable = true)\n",
    " - **num_mes**: decimal(20,0) (nullable = true)\n",
    " - **nom_mes**: string (nullable = true)\n",
    " - **regio_policial**: string (nullable = true)\n",
    " - **neighbourhood**: string (nullable = true)\n",
    " - **provincia**: string (nullable = true)\n",
    " - **comarca**: string (nullable = true)\n",
    " - **municipi**: string (nullable = true)\n",
    " - **tipus_de_lloc_dels_fets**: string (nullable = true)\n",
    " - **canal_dels_fets**: string (nullable = true)\n",
    " - **tipus_de_fet**: string (nullable = true)\n",
    " - **titol_del_fet_codi_penal**: string (nullable = true)\n",
    " - **tipus_de_fet_codi_penal**: string (nullable = true)\n",
    " - **ambit_fet**: string (nullable = true)\n",
    " - **nombre_fets_o_infraccions**: decimal(20,0) (nullable = true)\n",
    " - **nombre_victimes**: double (nullable = true)\n",
    "</div>\n",
    "\n",
    "\n",
    "Remove redundant columns: \n",
    "\n",
    "In this step, we remove the redundant columns from the DataFrame that are not of interest for our analysis. These columns include: \n",
    "\n",
    "provincia, comarca, municipi, regio_policial: Donen info que es igual que la de heighbourhood. \n",
    "nombre_fets_o_infraccions: Ca\n",
    "tipus_de_fet: Delictes\n",
    "canal_dels_fets: Presencial\n",
    "titol_del_fet_codi_penal:  Lesions\n",
    "\n",
    "All the regions, provinces, and comarcas are located within Barcelona.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 09:58:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/06/03 09:58:20 INFO SharedState: Warehouse path is 'file:/Users/elenaalegretregalado/Desktop/en_curs/Semantic_Data_Management-AI/data_preparation_pipeline/spark-warehouse'.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o32.load.\n: java.lang.ClassNotFoundException: org.duckdb.DuckDBDriver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zw/r9ly271s6rq9xxqqj4ldc_740000gn/T/ipykernel_87903/512845356.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_criminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjdbc_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SELECT * FROM df_criminal_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     def json(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.load.\n: java.lang.ClassNotFoundException: org.duckdb.DuckDBDriver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df_criminal = spark.read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", jdbc_url) \\\n",
    "  .option(\"driver\", driver) \\\n",
    "  .option(\"query\", \"SELECT * FROM df_criminal_dataset\") \\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"provincia\", \"comarca\", \"municipi\", \"regio_policial\"]\n",
    "df_criminal = df_criminal.drop(*columns_to_drop)\n",
    "# Remove redundant col --> nombre_fets_o_infraccions\n",
    "df_criminal = df_criminal.drop('nombre_fets_o_infraccions')\n",
    "df_criminal = df_criminal.drop('tipus_de_fet')\n",
    "\n",
    "# Drop canals_dels_fets --> Redundant\n",
    "df_criminal =  df_criminal.drop('canal_dels_fets')\n",
    "# Remove redundant col --> nombre_fets_o_infraccions\n",
    "df_criminal = df_criminal.drop('titol_del_fet_codi_penal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
